{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from Bio import SeqIO\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 氨基酸序列的双联体ONE-HOT编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'ACDEFGHIKLMNPQRSTVWYX'\n",
    "daa = []\n",
    "for x in alphabet:\n",
    "    for y in alphabet:\n",
    "        daa.append(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把一个长度为L的氨基酸序列转换为一个矩阵型数组。按照20个氨基酸的两联体以ONE-HOT方式编码\n",
    "#矩阵的列数是441\n",
    "#矩阵的行数根据参数r，有：(L-1)+(L-2)+...+(L-r)=rL-(r+1)*r/2\n",
    "def seq2DaaOneHotArray(sequence, r):\n",
    "    L = len(sequence)\n",
    "    N = r*L - ((r+1)*r)/2\n",
    "    result = np.zeros(shape=(int(N), 441))\n",
    "    m = 0\n",
    "    for i in range(r):\n",
    "        for j in range(L-i-1):\n",
    "            aa = sequence[j]+sequence[j+i+1]\n",
    "            k = daa.index(aa)\n",
    "            result[m][k] = 1\n",
    "            m = m + 1  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 根据序列的fasta文件构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入序列文件和位点文件\n",
    "def loadBindingsites(fastaFile, siteFile):\n",
    "    #读序列文件，每一个序列构成字典的一项，\n",
    "    #key：序列的id\n",
    "    #value: 氨基酸序列的字母字符串\n",
    "    data = {}\n",
    "    for seq_record in SeqIO.parse(fastaFile, 'fasta'):\n",
    "        data[seq_record.id] = seq_record.seq\n",
    "    \n",
    "    #读位点文件\n",
    "    bindingsites = {}\n",
    "    with open(siteFile, 'r') as pbsreader:\n",
    "        i = 0\n",
    "        for line in pbsreader:\n",
    "            i = i + 1\n",
    "            line = line.strip()\n",
    "            if '>' in line:\n",
    "                sid = line[1:]\n",
    "            else:\n",
    "                sites = line.split()\n",
    "                bs = []\n",
    "                for site in sites:\n",
    "                    bs.append( int(site))\n",
    "            if i%2 == 0:\n",
    "                bindingsites[sid] = bs\n",
    "      \n",
    "    return (data, bindingsites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,bindingsites = loadBindingsites('../data/PDNA-224.fasta','../data/PDNA-224-binding-sites.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把binding site和not binding site分别以稀疏矩阵存放在正负数据集中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建序列样本集\n",
    "def splitDatasets(data, bindingsites,ws,r):\n",
    "    positive=[]\n",
    "    negative=[]\n",
    "    for key in data:\n",
    "        sites = bindingsites[key]\n",
    "        p = data[key]\n",
    "        seqlen = len(p)\n",
    "        for j in range(seqlen):\n",
    "            if j < ws:\n",
    "                seq = str(p[j-ws:]) + str(p[0: ws+j+1])\n",
    "            elif j > seqlen - ws -1:\n",
    "                seq = str(p[j-ws:j]) + str(p[j:]) + str(p[0:ws-seqlen+j+1])\n",
    "            else:\n",
    "                seq = str(p[j-ws:j+ws+1])\n",
    "            m = seq2DaaOneHotArray(seq, r)\n",
    "            sm = coo_matrix(m)\n",
    "            if j in sites:\n",
    "                positive.append(sm)\n",
    "            else:\n",
    "                negative.append(sm)\n",
    "    \n",
    "    #positive = np.array(positive)\n",
    "    #negative = np.array(negative)\n",
    "    return (positive, negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive,negative = splitDatasets(data, bindingsites,15,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义CNN共享函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义权值\n",
    "def weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape, stddev=0.1), name='weight')\n",
    "\n",
    "#定义偏置\n",
    "def bias(shape):\n",
    "    return tf.Variable(tf.constant(0.1, shape=shape), name='bias')\n",
    "\n",
    "#定义卷积操作\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "#定义最大池化操作\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "#定义平均池化操作\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义CNN网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train，x_test是scipy的coo_matrix稀疏矩阵对象\n",
    "def cnn(x_train_pos, x_train_neg, x_test_pos, x_test_neg):\n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    #输入层 345-by-441\n",
    "    with tf.name_scope('input_layer'):\n",
    "        x = tf.placeholder(tf.float32,shape=[None, 345, 441, 1], name='x')\n",
    "    #第1个卷积层\n",
    "    with tf.name_scope('conv_1'):\n",
    "        W1 = weight([3,3,1,32])\n",
    "        b1 = bias([32])\n",
    "        conv_1 = conv2d(x, W1) + b1\n",
    "        conv_2 = tf.nn.relu(conv_1)\n",
    "\n",
    "    #第1个池化层 16-by-11\n",
    "    with tf.name_scope('pool_1'):\n",
    "        pool_1 = avg_pool_2x2(conv_1)\n",
    "\n",
    "    #第2个卷积层\n",
    "    with tf.name_scope('conv_2'):\n",
    "        W2 = weight([3,3,32,64])\n",
    "        b2 = bias([64])\n",
    "        conv_2 = conv2d(pool_1, W2) + b2\n",
    "        conv_2 = tf.nn.relu(conv_2)\n",
    "\n",
    "    #第2个池化层 8-by-6\n",
    "    with tf.name_scope(\"pool_2\"):\n",
    "        pool_2 = avg_pool_2x2(conv_2)\n",
    "\n",
    "    #第3个卷积层\n",
    "    with tf.name_scope('conv_3'):\n",
    "        W3 = weight([3,3,64,128])\n",
    "        b3 = bias([128])\n",
    "        conv_3 = conv2d(pool_2, W3) + b3\n",
    "        conv_3 = tf.nn.relu(conv_3)\n",
    "\n",
    "    #第3个池化层 4-by-3\n",
    "    with tf.name_scope('pool_3'):\n",
    "        pool_3 = avg_pool_2x2(conv_3)\n",
    "\n",
    "    #全连接层\n",
    "    with tf.name_scope('fc'):\n",
    "        #将最后一个池化层的128个通道的4-by-3的图像转换为一维向量，长度是128*4*3=1536\n",
    "        W4 = weight([1536,256]) #全连接层定义256个神经元\n",
    "        b4 = bias([256])\n",
    "        flat = tf.reshape(pool_3, [-1, 1536])\n",
    "        h = tf.nn.relu(tf.matmul(flat, W4)) + b4\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "        h_dropout = tf.nn.dropout(h, keep_prob)\n",
    "\n",
    "    #输出层\n",
    "    with tf.name_scope('output_layer'):\n",
    "        W5 = weight([256,2])\n",
    "        b5 = bias([2])\n",
    "        pred = tf.nn.softmax(tf.matmul(h_dropout, W5) + b5)\n",
    "    \n",
    "    #构建网络模型\n",
    "    with tf.name_scope(\"optimizer\"):\n",
    "        #定义占位符\n",
    "        y = tf.placeholder(tf.int32, shape=[None, 2], name=\"label\")\n",
    "        #定义损失函数\n",
    "        loss_function = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred,\n",
    "                                                                                 labels=y))\n",
    "        #选择优化器\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.003).minimize(loss_function)\n",
    "    \n",
    "    #定义准确率\n",
    "    with tf.name_scope(\"evalulation\"):\n",
    "        correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    #训练模型\n",
    "    X_TRAIN_SIZE = len(x_train)\n",
    "    TRAIN_EPOCHES = 25\n",
    "    BATCH_SIZE = 50\n",
    "    TOTAL_BATCH = int( np.ceil( X_TRAIN_SIZE / BATCH_SIZE))\n",
    "    epoch= tf.Variable(0, name='epoch', trainable=False)\n",
    "    STARTTIME = time()\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        '''\n",
    "        # 设置检查点存储目录\n",
    "        ckpt_dir = \"../log/\"\n",
    "        if not os.path.exists(ckpt_dir):\n",
    "            os.makedirs(ckpt_dir)\n",
    "        #生成saver\n",
    "        saver = tf.train.Saver(max_to_keep=5)\n",
    "        # 创建 summary_writer，用于写图文件\n",
    "        summary_writer = tf.summary.FileWriter(ckpt_dir, sess.graph)\n",
    "        # 如果有检查点文件，读取最新的检查点文件，恢复各种变量值\n",
    "        ckpt = tf.train.latest_checkpoint(ckpt_dir )\n",
    "        if ckpt != None:\n",
    "            saver.restore(sess, ckpt)     \n",
    "        else:\n",
    "            print(\"Training from scratch.\")\n",
    "\n",
    "        start_epoch= sess.run(epoch)\n",
    "        print(\"Training starts form {} epoch.\".format(start_epoch+1))\n",
    "        '''\n",
    "        #迭代训练\n",
    "        for ep in range(start_epoch, start_epoch + TRAIN_EPOCHES):\n",
    "            for i in range(TOTAL_BATCH):\n",
    "                start = (i * BATCH_SIZE) % X_TRAIN_SIZE\n",
    "                end = min(start + BATCH_SIZE, X_TRAIN_SIZE)\n",
    "                batch_x = x_train[start:end]\n",
    "                batch_y = y_train[start:end]\n",
    "                sess.run(optimizer,feed_dict={x: batch_x, y: batch_y, keep_prob:0.7})\n",
    "                if i % 100 == 0:\n",
    "                    print(\"Step {}\".format(i), \"finished\")\n",
    "\n",
    "            loss,acc = sess.run([loss_function,accuracy],feed_dict={x: batch_x, y: batch_y, keep_prob:0.7})\n",
    "            \n",
    "            print(\"Train epoch:\", '%02d' % (sess.run(epoch)+1), \\\n",
    "                  \"Loss=\",\"{:.6f}\".format(loss),\" Accuracy=\",acc)\n",
    "\n",
    "            #保存检查点\n",
    "            saver.save(sess,ckpt_dir+\"DBPSite_cnn_model.cpkt\",global_step=ep+1)\n",
    "\n",
    "            sess.run(epoch.assign(ep+1))\n",
    "    \n",
    "        duration =time()-STARTTIME\n",
    "        print(\"Train finished takes:\",duration)   \n",
    "    \n",
    "        #计算测试集上的预测结果\n",
    "        y_pred = sess.run(pred, feed_dict={x: x_test, y: y_test, keep_prob: 1.0})\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对数据集X（list类型）按KFold划分训练集和测试集(list类型)\n",
    "def splitByKFold(X, n=5):\n",
    "    x_trains = []\n",
    "    x_tests = []\n",
    "    kf = KFold(n_splits=n)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        x_train, x_test = [],[]\n",
    "        for k in train_index:\n",
    "            x_train.append(X[k])\n",
    "        for k in test_index:\n",
    "            x_test.append(x[k])\n",
    "        x_trains.append(x_train)\n",
    "        x_tests.append(x_test)\n",
    "     \n",
    "    return x_trains, x_tests\n",
    "\n",
    "#x_positive,x_negative都是list类型\n",
    "def validationKFold(x_positive, x_negative, n=5):\n",
    "    x_trains_pos, x_tests_pos = splitByKFold(x_positive, n)\n",
    "    x_trains_neg, x_tests_neg = splitByKFold(x_negative, n)\n",
    "    \n",
    "    y_logists = np.ndarray((0,2))\n",
    "    y_preds = np.ndarray((0,2))\n",
    "    for i in range(n):            \n",
    "        y_pred = cnn(x_trains_pos[i], x_trains_neg[i], x_tests_pos[i], x_tests_neg[i])\n",
    "        \n",
    "        k_test_pos = int( len( x_test_pos)) #正类测试样本数\n",
    "        k_test_neg = int( len( x_test_neg)) #负类测试样本数\n",
    "        y_test_pos = np.tile([1,0], (k_test_pos,1))\n",
    "        y_test_neg = np.tile([0,1], (k_test_neg,1))\n",
    "        \n",
    "        y_test = np.append(y_test_pos, y_test_neg, axis=0)\n",
    "        \n",
    "        y_logists = np.append(y_logists, y_test, axis=0)\n",
    "        y_preds = np.append(y_preds, y_pred, axis=0)\n",
    "        \n",
    "        return y_logists, y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-211b9586740f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_tests\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplitByKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-bf879c702d48>\u001b[0m in \u001b[0;36msplitByKFold\u001b[1;34m(X, n)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mx_trains\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mx_tests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "x_train, x_tests = splitByKFold(positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "*************\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "*************\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "*************\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "*************\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "x_trains = []\n",
    "x_tests = []\n",
    "X = positive[:20]\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"*************\")\n",
    "    x_train = []\n",
    "    for k in train_index:\n",
    "        print(k)\n",
    "        x_train.append(positive[k])\n",
    "    x_trains.append(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-27-e884af6036c0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-27-e884af6036c0>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    XX=[x_trains[1] x_trains[0]]\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "XX=[x_trains[1] x_trains[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>,\n",
       " <345x441 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 345 stored elements in COOrdinate format>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_trains[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
